name: Deduplicate Archive

on:
  workflow_dispatch:

jobs:
  deduplicate:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Create dedup script
        run: |
          cat > dedup.py << 'ENDOFSCRIPT'
          import os
          import hashlib
          from pathlib import Path
          from collections import defaultdict
          
          PRIORITY = {'.md': 5, '.html': 4, '.txt': 3, '.docx': 2, '.pdf': 1, '.zip': 0}
          
          def get_hash(filepath):
              sha256 = hashlib.sha256()
              try:
                  with open(filepath, 'rb') as f:
                      for chunk in iter(lambda: f.read(8192), b''):
                          sha256.update(chunk)
                  return sha256.hexdigest()
              except:
                  return None
          
          def get_priority(filepath):
              ext = Path(filepath).suffix.lower()
              return PRIORITY.get(ext, -1)
          
          print("Scanning repository...")
          files_by_hash = defaultdict(list)
          total = 0
          
          for root, dirs, files in os.walk('.'):
              if '.git' in root:
                  continue
              for file in files:
                  fp = os.path.join(root, file)
                  h = get_hash(fp)
                  if h:
                      files_by_hash[h].append(fp)
                      total += 1
          
          print(f"Total files: {total}")
          
          duplicates = {h: f for h, f in files_by_hash.items() if len(f) > 1}
          
          if not duplicates:
              print("No duplicates found")
              exit(0)
          
          print(f"Duplicate groups: {len(duplicates)}")
          
          report = ["# Duplicate Files Report\n\n"]
          to_delete = []
          
          for h, fps in duplicates.items():
              sorted_fps = sorted(fps, key=get_priority, reverse=True)
              keep = sorted_fps[0]
              delete = sorted_fps[1:]
              
              report.append(f"## Hash: {h[:16]}\n")
              report.append(f"Keep: {keep}\n")
              report.append(f"Delete: {len(delete)}\n")
              for d in delete:
                  report.append(f"- {d}\n")
                  to_delete.append(d)
              report.append("\n")
          
          with open('DUPLICATE_REPORT.md', 'w') as f:
              f.writelines(report)
          
          print(f"Marked for deletion: {len(to_delete)}")
          
          for fp in to_delete:
              try:
                  os.remove(fp)
                  print(f"Deleted: {fp}")
              except Exception as e:
                  print(f"Failed: {fp} - {e}")
          
          print(f"Cleanup complete. Removed: {len(to_delete)}")
          ENDOFSCRIPT
      
      - name: Run deduplication
        run: python3 dedup.py
      
      - name: Commit changes
        run: |
          git config user.email "action@github.com"
          git config user.name "GitHub Action"
          git add -A
          git commit -m "chore: remove duplicates" || echo "No changes"
          git push
